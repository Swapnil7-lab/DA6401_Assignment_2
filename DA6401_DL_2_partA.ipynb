{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swapnil7-lab/DA6401_Assignment_2/blob/main/DA6401_DL_2_partA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d1706649",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:18:55.180502Z",
          "iopub.status.busy": "2023-04-07T11:18:55.179687Z",
          "iopub.status.idle": "2023-04-07T11:18:58.147235Z",
          "shell.execute_reply": "2023-04-07T11:18:58.146104Z"
        },
        "papermill": {
          "duration": 2.974913,
          "end_time": "2023-04-07T11:18:58.150711",
          "exception": false,
          "start_time": "2023-04-07T11:18:55.175798",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1706649",
        "outputId": "c95da646-4f8d-4d7b-d4c8-b6325bd14185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "2.6.0+cu124\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "print(torch.device('cuda:0'))\n",
        "print(torch.__version__)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bfc3bb19",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:18:58.158540Z",
          "iopub.status.busy": "2023-04-07T11:18:58.158066Z",
          "iopub.status.idle": "2023-04-07T11:19:45.932331Z",
          "shell.execute_reply": "2023-04-07T11:19:45.930839Z"
        },
        "papermill": {
          "duration": 47.781297,
          "end_time": "2023-04-07T11:19:45.935224",
          "exception": false,
          "start_time": "2023-04-07T11:18:58.153927",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfc3bb19",
        "outputId": "d076e9ab-998d-4054-fbf9-d2c050c0336b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-03 04:19:56--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.31.207, 142.251.111.207, 142.251.16.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.31.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3816687935 (3.6G) [application/zip]\n",
            "Saving to: ‘nature_12K.zip’\n",
            "\n",
            "nature_12K.zip      100%[===================>]   3.55G  25.9MB/s    in 37s     \n",
            "\n",
            "2025-04-03 04:20:33 (98.5 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://storage.googleapis.com/wandb_datasets/nature_12K.zip'\n",
        "!unzip -q nature_12K.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "edd29c7e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:19:47.710012Z",
          "iopub.status.busy": "2023-04-07T11:19:47.701577Z",
          "iopub.status.idle": "2023-04-07T11:19:48.137919Z",
          "shell.execute_reply": "2023-04-07T11:19:48.136900Z"
        },
        "papermill": {
          "duration": 0.501656,
          "end_time": "2023-04-07T11:19:48.140529",
          "exception": false,
          "start_time": "2023-04-07T11:19:47.638873",
          "status": "completed"
        },
        "tags": [],
        "id": "edd29c7e"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "# Standard library imports\n",
        "import os\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "# Third-party library imports\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Torchvision imports\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "# Define the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# Data Preparation and Transformation\n",
        "def load_data(bs, augment_data=False):\n",
        "    # Configuration parameters\n",
        "    img_size = (300, 300)\n",
        "    norm_mean = (0.5, 0.5, 0.5)\n",
        "    norm_std = (0.5, 0.5, 0.5)\n",
        "\n",
        "    # Base image transformations\n",
        "    base_transform = [\n",
        "        transforms.Resize(img_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(norm_mean, norm_std)\n",
        "    ]\n",
        "\n",
        "    # Augmentation additions\n",
        "    augmentation_layers = [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10)\n",
        "    ] if augment_data else []\n",
        "\n",
        "    # Create transformation pipelines\n",
        "    train_transforms = transforms.Compose([*augmentation_layers, *base_transform])\n",
        "    test_transforms = transforms.Compose(base_transform)\n",
        "\n",
        "    # Dataset paths configuration\n",
        "    data_root = \"/content/inaturalist_12K\"\n",
        "    train_dir = os.path.join(data_root, 'train')\n",
        "    eval_dir = os.path.join(data_root, 'val')\n",
        "\n",
        "    # Dataset preparation\n",
        "    train_full = ImageFolder(train_dir, transform=train_transforms)\n",
        "    eval_set = ImageFolder(eval_dir, transform=test_transforms)\n",
        "\n",
        "    # Data partitioning\n",
        "    total_train = len(train_full)\n",
        "    val_portion = 0.2\n",
        "    train_samples = int(total_train * (1 - val_portion))\n",
        "    val_samples = total_train - train_samples\n",
        "\n",
        "    # Dataset splitting\n",
        "    train_subset, val_subset = random_split(train_full, [train_samples, val_samples])\n",
        "\n",
        "    # Data loading configuration\n",
        "    loader_config = {\n",
        "        'batch_size': bs,\n",
        "        'num_workers': 2,\n",
        "        'pin_memory': True\n",
        "    }\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_subset, shuffle=True, **loader_config)\n",
        "    val_loader = DataLoader(val_subset, shuffle=False, **loader_config)\n",
        "    test_loader = DataLoader(eval_set, shuffle=False, **loader_config)\n",
        "\n",
        "    # Class label extraction\n",
        "    class_labels = [item.name for item in pathlib.Path(train_dir).iterdir()]\n",
        "    class_labels.sort()\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_labels\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7c5f5f5b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:19:48.157834Z",
          "iopub.status.busy": "2023-04-07T11:19:48.157506Z",
          "iopub.status.idle": "2023-04-07T11:19:48.181396Z",
          "shell.execute_reply": "2023-04-07T11:19:48.180429Z"
        },
        "papermill": {
          "duration": 0.0347,
          "end_time": "2023-04-07T11:19:48.183608",
          "exception": false,
          "start_time": "2023-04-07T11:19:48.148908",
          "status": "completed"
        },
        "tags": [],
        "id": "7c5f5f5b"
      },
      "outputs": [],
      "source": [
        "#Simple CNN\n",
        "def flatten(k=[11, 9, 7, 5, 3], w=300, s=1, p=1):\n",
        "    r = w\n",
        "    i = 0  # Initialize the counter for the while loop\n",
        "\n",
        "    while i < len(k):  # Loop until the counter reaches the length of k\n",
        "        print(\"r\", r)\n",
        "        r = (r + 2 * p - k[i]) + 1\n",
        "        r = int(r / 2) + 1\n",
        "        i += 1  # Increment the counter\n",
        "\n",
        "    return r\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_class=10, num_filters=4, kernel_sizes=[11,9,7,5,3],\n",
        "                 fc_neurons=64, batch_norm=True, dropout=0.3, filter_multiplier=2, activation='LeakyRelu'):\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        # Preserve original parameter assignments\n",
        "        self.in_channels = in_channels\n",
        "        self.num_class = num_class\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_sizes = kernel_sizes\n",
        "        self.fc_neurons = fc_neurons\n",
        "        self.activation = activation\n",
        "        self.batch_norm = batch_norm\n",
        "        self.dropout = dropout\n",
        "        self.filter_multiplier = filter_multiplier\n",
        "\n",
        "        # Layer construction through systematic pattern\n",
        "        prev_channels = in_channels\n",
        "        for layer_idx in range(len(kernel_sizes)):\n",
        "            # Calculate output channels using exponential growth\n",
        "            out_channels = num_filters * (filter_multiplier ** layer_idx)\n",
        "\n",
        "            # Convolutional block components\n",
        "            setattr(self, f'conv{layer_idx+1}', nn.Conv2d(\n",
        "                prev_channels, out_channels,\n",
        "                kernel_size=kernel_sizes[layer_idx],\n",
        "                stride=1,\n",
        "                padding=1\n",
        "            ).to(device))\n",
        "\n",
        "            if batch_norm:\n",
        "                setattr(self, f'bn{layer_idx+1}', nn.BatchNorm2d(out_channels))\n",
        "\n",
        "            setattr(self, f'relu{layer_idx+1}', nn.LeakyReLU())\n",
        "            setattr(self, f'pool{layer_idx+1}', nn.MaxPool2d(2, 2, padding=1))\n",
        "\n",
        "            prev_channels = out_channels\n",
        "\n",
        "        # Calculate spatial dimension reduction\n",
        "        self.r = flatten(kernel_sizes)\n",
        "        print(\"ok pool5\")\n",
        "        print(\"ok flatten\")\n",
        "        print(self.r)\n",
        "\n",
        "        # Fully connected section with dynamic sizing\n",
        "        final_channels = num_filters * (filter_multiplier ** (len(kernel_sizes)-1))\n",
        "        self.fc1 = nn.Linear(\n",
        "            final_channels * self.r * self.r,\n",
        "            fc_neurons\n",
        "        )\n",
        "        self.relu6 = nn.LeakyReLU()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(fc_neurons, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Unified processing loop for convolutional blocks\n",
        "        for block_idx in range(1, 6):\n",
        "            x = getattr(self, f'conv{block_idx}')(x)\n",
        "            if self.batch_norm:\n",
        "                x = getattr(self, f'bn{block_idx}')(x)\n",
        "            x = getattr(self, f'relu{block_idx}')(x)\n",
        "            x = getattr(self, f'pool{block_idx}')(x)\n",
        "\n",
        "        # Flatten and classify\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu6(self.fc1(x))\n",
        "        return self.fc2(self.drop(x))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "173205f4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:19:48.200190Z",
          "iopub.status.busy": "2023-04-07T11:19:48.199914Z",
          "iopub.status.idle": "2023-04-07T11:50:32.615877Z",
          "shell.execute_reply": "2023-04-07T11:50:32.614873Z"
        },
        "papermill": {
          "duration": 1844.430252,
          "end_time": "2023-04-07T11:50:32.621794",
          "exception": false,
          "start_time": "2023-04-07T11:19:48.191542",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "173205f4",
        "outputId": "c23e8802-6432-414d-cf86-e79287621405"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.DS_Store', 'Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n",
            "Feature Batch Shape: torch.Size([64, 3, 300, 300])\n",
            "Label Batch Shape: torch.Size([64])\n",
            "r 300\n",
            "r 151\n",
            "r 76\n",
            "r 39\n",
            "r 20\n",
            "ok pool5\n",
            "ok flatten\n",
            "11\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-dc9af1628a8e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Forward pass through the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-8455ff3f635b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Unified processing loop for convolutional blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'conv{block_idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'bn{block_idx}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Configure device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define hyperparameters\n",
        "in_channels = 3\n",
        "num_class = 10\n",
        "learning_rate = 0.0005\n",
        "batch_size = 64\n",
        "epochs = 15\n",
        "data_aug = True\n",
        "\n",
        "# Load dataset\n",
        "train_loader, val_loader, test_loader, classes = load_data(batch_size, data_aug)\n",
        "print(classes)\n",
        "\n",
        "# Display a batch of training data\n",
        "trainfeature, trainlabel = next(iter(train_loader))\n",
        "print(f\"Feature Batch Shape: {trainfeature.size()}\")\n",
        "print(f\"Label Batch Shape: {trainlabel.size()}\")\n",
        "\n",
        "# Initialize the model\n",
        "model = CNN(in_channels, num_class, 16, [3, 3, 3, 3, 3], 128, False, 0, 2, 'LeakyRelu').to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.NAdam(model.parameters(), lr=learning_rate, weight_decay=0.0001)\n",
        "\n",
        "# Training loop using while\n",
        "epoch = 0\n",
        "while epoch < epochs:\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    train_iter = iter(train_loader)\n",
        "    batch_idx = 0\n",
        "\n",
        "    while batch_idx < len(train_loader):\n",
        "        # Get the next batch of data and targets\n",
        "        data, targets = next(train_iter)\n",
        "\n",
        "        # Transfer data to the appropriate device\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Zero out gradients from the previous step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the model\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_idx += 1\n",
        "\n",
        "    # Evaluation mode for validation/testing\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    test_iter = iter(test_loader)\n",
        "    test_idx = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while test_idx < len(test_loader):\n",
        "            # Get the next batch of validation/testing data and targets\n",
        "            data, targets = next(test_iter)\n",
        "\n",
        "            # Transfer data to the appropriate device\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass for validation/testing\n",
        "            scores = model(data)\n",
        "            test_loss += criterion(scores, targets).item()\n",
        "\n",
        "            # Calculate predictions and accuracy\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == targets).sum().item()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "            test_idx += 1\n",
        "\n",
        "    # Compute average loss and accuracy for validation/testing\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = num_correct / num_samples\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.4f}, '\n",
        "          f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc * 100:.2f}%')\n",
        "\n",
        "    epoch += 1\n",
        "\n",
        "# Save the best model to a file\n",
        "best_model_path = 'best_model.pth'\n",
        "torch.save(model.state_dict(), best_model_path)\n",
        "print(f\"Best model saved to {best_model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "in_channels = 3\n",
        "num_class = 10\n",
        "learning_rate = 0.0001\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "data_aug=False\n",
        "\n",
        "# Load data\n",
        "train_loader,val_loader,test_loader,classes=load_data(batch_size,data_aug)\n",
        "print(classes)\n",
        "trainfeature, trainlabel = next(iter(train_loader))\n",
        "print(f\"Feature Batch Shape: {trainfeature.size()}\")\n",
        "print(f\"Label Batch Shape: {trainlabel.size()}\")\n",
        "\n",
        "\n",
        "\n",
        "# Initialize network\n",
        "model = CNN(3,10,16,[7,5,5,3,3],64,True,0.2,2,'Mish').to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer=optim.NAdam(model.parameters(),lr=learning_rate,weight_decay=0.0001)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # backward\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Track the total loss and number of correct predictions\n",
        "    test_loss = 0\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "\n",
        "            scores = model(data)\n",
        "            test_loss += criterion(scores, targets).item()\n",
        "\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == targets).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    # Calculate the average validation loss and accuracy\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = float(num_correct) / num_samples\n",
        "\n",
        "    # Print the epoch number, loss, and accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, epochs, loss.item(), test_loss, test_acc*100))\n",
        "\n",
        "# Check accuracy on training & test to see how good our model\n",
        "# Save best model\n",
        "best_model_path = 'best_model.pth'\n",
        "torch.save(model.state_dict(), best_model_path)\n",
        "print(f\"Best model saved to {best_model_path}\")"
      ],
      "metadata": {
        "id": "Uu9Xrbjq55Io"
      },
      "id": "Uu9Xrbjq55Io",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the best model and testing it on Test Data\n",
        "best_model_path = 'best_model.pth'\n",
        "\n",
        "loaded_model = CNN(3,10,16,[7,5,5,3,3],64,True,0.2,2,'Mish').to(device)\n",
        "loaded_model.load_state_dict(torch.load(best_model_path)) # it takes the loaded dictionary, not the path file itself\n",
        "\n",
        "def calculate_accuracy(model, test_loader,criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    cost=0\n",
        "    acc=0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            cost +=criterion(outputs,labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images\n",
        "            del labels\n",
        "    acc=100 * correct / total\n",
        "    cost/=len(test_loader)\n",
        "\n",
        "    return cost,acc\n",
        "loss,acc=calculate_accuracy(loaded_model,test_loader,nn.CrossEntropyLoss())\n",
        "print(loss,acc)\n",
        "\n",
        "print(loaded_model.state_dict())\n"
      ],
      "metadata": {
        "id": "DlB9WFER0we3"
      },
      "id": "DlB9WFER0we3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path = 'best_model.pth'\n",
        "\n",
        "loaded_model = CNN(3,10,16,[7,5,5,3,3],64,True,0.2,2,'Mish')\n",
        "loaded_model.load_state_dict(torch.load(best_model_path)) # it takes the loaded dictionary, not the path file itself\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"CS6910_Assignment_2_Q2\")\n",
        "\n",
        "\n",
        "# Define a function to generate predictions and sample images from the test data\n",
        "def generate_predictions(model, data_loader):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a list to store the predictions and sample images\n",
        "    predictions = []\n",
        "    sample_images = []\n",
        "\n",
        "    # Generate predictions and sample images\n",
        "    with torch.no_grad():\n",
        "        for batch, _ in data_loader:\n",
        "            # Forward pass through the model\n",
        "            output = model(batch)\n",
        "\n",
        "            # Get the predicted class labels\n",
        "            _, predicted = torch.max(output, 1)\n",
        "\n",
        "            # Convert the predicted labels to image tensors\n",
        "            predicted_images = torchvision.utils.make_grid(batch[predicted])\n",
        "\n",
        "            # Append the predictions and sample images to the lists\n",
        "            predictions.append(predicted_images)\n",
        "            sample_images.append(torchvision.utils.make_grid(batch))\n",
        "\n",
        "    # Concatenate the predictions and sample images into grids\n",
        "    prediction_grid = torchvision.utils.make_grid(predictions, nrow=3)\n",
        "    sample_grid = torchvision.utils.make_grid(sample_images, nrow=3)\n",
        "\n",
        "    # Return the grids\n",
        "    return prediction_grid, sample_grid\n",
        "\n",
        "# Generate the prediction and sample image grids\n",
        "prediction_grid, sample_grid = generate_predictions(loaded_model, test_loader)\n",
        "\n",
        "# Log the grids to wandb\n",
        "wandb.log({\n",
        "    'Predictions': wandb.Image(prediction_grid),\n",
        "    'Sample Images': wandb.Image(sample_grid)\n",
        "})\n",
        "\n",
        "# Finish the run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "yoVD_qthFg6D"
      },
      "id": "yoVD_qthFg6D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703c4d76",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:50:32.649484Z",
          "iopub.status.busy": "2023-04-07T11:50:32.649023Z",
          "iopub.status.idle": "2023-04-07T11:50:52.669256Z",
          "shell.execute_reply": "2023-04-07T11:50:52.667908Z"
        },
        "papermill": {
          "duration": 20.037041,
          "end_time": "2023-04-07T11:50:52.672195",
          "exception": false,
          "start_time": "2023-04-07T11:50:32.635154",
          "status": "completed"
        },
        "tags": [],
        "id": "703c4d76"
      },
      "outputs": [],
      "source": [
        "\n",
        "from signal import signal,SIGPIPE, SIG_DFL\n",
        "signal(SIGPIPE,SIG_DFL)\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "!wandb login --relogin 3d199b9bde866b3494cda2f8bb7c7a633c9fdade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be445fb4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:50:52.695381Z",
          "iopub.status.busy": "2023-04-07T11:50:52.695007Z",
          "iopub.status.idle": "2023-04-07T11:50:52.715754Z",
          "shell.execute_reply": "2023-04-07T11:50:52.714604Z"
        },
        "papermill": {
          "duration": 0.033186,
          "end_time": "2023-04-07T11:50:52.717717",
          "exception": false,
          "start_time": "2023-04-07T11:50:52.684531",
          "status": "completed"
        },
        "tags": [],
        "id": "be445fb4"
      },
      "outputs": [],
      "source": [
        "\n",
        "sweep_config = {\n",
        "    \"name\" : \"DA6401_Assignment_2\",\n",
        "    \"method\" : \"bayes\",\n",
        "    'metric': {\n",
        "        'name': 'val_acc',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    \"parameters\" : {\n",
        "        \"optimizer\" : {\n",
        "            \"values\" : ['adam','nadam','sgd']\n",
        "        },\n",
        "        \"activation\" : {\n",
        "            \"values\" : ['LeakyRelu','Selu','Gelu','Mish']\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            \"values\": [0.001,0.0001,0.0003,0.0005]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0,0.2,0.3]\n",
        "        },\n",
        "        \"batch_norm\": {\n",
        "              \"values\": [True,False]\n",
        "        },\n",
        "        \"data_aug\": {\n",
        "              \"values\": [True,False]\n",
        "        },\n",
        "        'kernel_sizes':{\n",
        "            'values': [[3,3,3,3,3],[5,5,5,5,5],[7,5,5,3,3], [11,9,7,5,3]]\n",
        "        },\n",
        "        'filter_multiplier': {\n",
        "            'values': [1, 2, 0.5]\n",
        "        },\n",
        "        'num_filters': {\n",
        "            'values': [4,8,16]\n",
        "        },\n",
        "        \"fc_neurons\": {\n",
        "              \"values\": [32, 64, 128]\n",
        "          }\n",
        "    }\n",
        "}\n",
        "def opti(model,opt='adam',lr=0.0005):\n",
        "    print(\"in opti\")\n",
        "    if opt == \"sgd\":\n",
        "        opt= optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    elif opt == \"adam\":\n",
        "        opt = optim.Adam(model.parameters(),lr=lr,weight_decay=0.0001)\n",
        "    elif opt == \"nadam\":\n",
        "        opt = optim.NAdam(model.parameters(),lr=lr,weight_decay=0.0001)\n",
        "    print('exit opti')\n",
        "    return opt\n",
        "\n",
        "def calculate_accuracy(model, test_loader,criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    cost=0\n",
        "    acc=0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            cost +=criterion(outputs,labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images\n",
        "            del labels\n",
        "    acc=100 * correct / total\n",
        "    cost/=len(test_loader)\n",
        "\n",
        "    return cost,acc\n",
        "def train():\n",
        "    config_default={\n",
        "      'epochs':15,\n",
        "      'batch_size':32,\n",
        "      'learning_rate':0.001,\n",
        "      'dropout':0.3,\n",
        "      'batch_norm':True,\n",
        "      'data_aug':True,\n",
        "      'kernel_sizes':[5,5,5,5,5],\n",
        "      'filter_multiplier': 2,\n",
        "      'num_filters': 16,\n",
        "      \"fc_neurons\": 64\n",
        "  }\n",
        "    wandb.init(config=config_default)\n",
        "    c= wandb.config\n",
        "    name = \"nfliter_\"+str(c.num_filters)+\"op_\"+str(c.optimizer)+\"_ac_\"+str(c.activation)+\"_n_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)+\"_dp_\"+str(c.dropout)+\"_bn_\"+str(c.batch_norm)\n",
        "\n",
        "    wandb.init(name=name)\n",
        "\n",
        "    # Retrieve the hyperparameters from the config\n",
        "    lr = c.learning_rate\n",
        "    bs = c.batch_size\n",
        "    epochs = 15\n",
        "    act= c.activation\n",
        "    opt= c.optimizer\n",
        "\n",
        "    dp = c.dropout\n",
        "    bn = c.batch_norm\n",
        "    da=c.data_aug\n",
        "    ks=c.kernel_sizes\n",
        "    fm=c.filter_multiplier\n",
        "    nf=c.num_filters\n",
        "    fc=c.fc_neurons\n",
        "\n",
        "\n",
        "    # Load the dataset\n",
        "    train_loader,val_loader,test_loader,classes=load_data(bs,da)\n",
        "\n",
        "    print(\"data loaded ====================================================\")\n",
        "\n",
        "    # Initialize network\n",
        "    model= CNN(in_channels=3, num_class=10,num_filters=nf,kernel_sizes=ks,fc_neurons=fc,batch_norm=bn,dropout=dp,filter_multiplier=fm,activation=act).to(device)\n",
        "    print(\"model ini==============================================================\")\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer=opti(model,opt,lr)\n",
        "    print(\"done\")\n",
        "    # Train Network\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch enter')\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            # Get data to cuda if possible\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # forward\n",
        "            scores = model(data)\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # backward\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # gradient descent or adam step\n",
        "            optimizer.step()\n",
        "            del data\n",
        "            del targets\n",
        "\n",
        "        # Calculate the test accuracy\n",
        "        train_loss,train_acc = calculate_accuracy(model, train_loader,criterion)\n",
        "        val_loss,val_acc = calculate_accuracy(model, val_loader,criterion)\n",
        "        test_loss,test_acc = calculate_accuracy(model, test_loader,criterion)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        # Log the metrics to WandB\n",
        "        wandb.log({'epoch': epoch+1,'loss':loss.item(), 'train_loss': loss.item(),'test_loss':test_loss,'val_loss':val_loss,'test_acc': test_acc,'train_acc': train_acc,'val_acc': val_acc})\n",
        "\n",
        "\n",
        "    # Save the best model\n",
        "    wandb.save('model.h5')\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47928aa2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:50:52.735356Z",
          "iopub.status.busy": "2023-04-07T11:50:52.735029Z",
          "iopub.status.idle": "2023-04-07T17:11:27.821419Z",
          "shell.execute_reply": "2023-04-07T17:11:27.820420Z"
        },
        "papermill": {
          "duration": 19235.097974,
          "end_time": "2023-04-07T17:11:27.823909",
          "exception": false,
          "start_time": "2023-04-07T11:50:52.725935",
          "status": "completed"
        },
        "tags": [],
        "id": "47928aa2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_2')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25641c79",
      "metadata": {
        "papermill": {
          "duration": 0.084995,
          "end_time": "2023-04-07T17:11:27.931145",
          "exception": false,
          "start_time": "2023-04-07T17:11:27.846150",
          "status": "completed"
        },
        "tags": [],
        "id": "25641c79"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_2')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77dac26f",
      "metadata": {
        "id": "77dac26f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_2')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7393ba6e",
      "metadata": {
        "id": "7393ba6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_2')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a699f0",
      "metadata": {
        "id": "d2a699f0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 21165.436974,
      "end_time": "2023-04-07T17:11:30.816189",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-04-07T11:18:45.379215",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}