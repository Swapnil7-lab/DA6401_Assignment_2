{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swapnil7-lab/DA6401_Assignment_2/blob/main/DA6401_DL_2_partA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d1706649",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:18:55.180502Z",
          "iopub.status.busy": "2023-04-07T11:18:55.179687Z",
          "iopub.status.idle": "2023-04-07T11:18:58.147235Z",
          "shell.execute_reply": "2023-04-07T11:18:58.146104Z"
        },
        "papermill": {
          "duration": 2.974913,
          "end_time": "2023-04-07T11:18:58.150711",
          "exception": false,
          "start_time": "2023-04-07T11:18:55.175798",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1706649",
        "outputId": "c95da646-4f8d-4d7b-d4c8-b6325bd14185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "2.6.0+cu124\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "print(torch.device('cuda:0'))\n",
        "print(torch.__version__)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bfc3bb19",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:18:58.158540Z",
          "iopub.status.busy": "2023-04-07T11:18:58.158066Z",
          "iopub.status.idle": "2023-04-07T11:19:45.932331Z",
          "shell.execute_reply": "2023-04-07T11:19:45.930839Z"
        },
        "papermill": {
          "duration": 47.781297,
          "end_time": "2023-04-07T11:19:45.935224",
          "exception": false,
          "start_time": "2023-04-07T11:18:58.153927",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfc3bb19",
        "outputId": "649f21f6-8bdc-4da3-8eda-cc605b06f0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-03 03:57:34--  https://storage.googleapis.com/wandb_datasets/nature_12K.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.207, 142.251.2.207, 142.250.141.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3816687935 (3.6G) [application/zip]\n",
            "Saving to: ‘nature_12K.zip’\n",
            "\n",
            "nature_12K.zip      100%[===================>]   3.55G   135MB/s    in 33s     \n",
            "\n",
            "2025-04-03 03:58:07 (112 MB/s) - ‘nature_12K.zip’ saved [3816687935/3816687935]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://storage.googleapis.com/wandb_datasets/nature_12K.zip'\n",
        "!unzip -q nature_12K.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "edd29c7e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:19:47.710012Z",
          "iopub.status.busy": "2023-04-07T11:19:47.701577Z",
          "iopub.status.idle": "2023-04-07T11:19:48.137919Z",
          "shell.execute_reply": "2023-04-07T11:19:48.136900Z"
        },
        "papermill": {
          "duration": 0.501656,
          "end_time": "2023-04-07T11:19:48.140529",
          "exception": false,
          "start_time": "2023-04-07T11:19:47.638873",
          "status": "completed"
        },
        "tags": [],
        "id": "edd29c7e"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "# Standard library imports\n",
        "import os\n",
        "import random\n",
        "import pathlib\n",
        "\n",
        "# Third-party library imports\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Torchvision imports\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "# Define the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# Dataset Augmentation\n",
        "def load_data(bs,augment_data=False):\n",
        "    # define the transforms to be applied to the training data\n",
        "    if augment_data:\n",
        "        train_transforms = transforms.Compose([\n",
        "          transforms.Resize((300,300)),\n",
        "          transforms.RandomHorizontalFlip(),\n",
        "          transforms.RandomRotation(10),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "      ])\n",
        "    else:\n",
        "        train_transforms = transforms.Compose([\n",
        "          transforms.Resize((300,300)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "      ])\n",
        "\n",
        "    test_transforms = transforms.Compose([\n",
        "      transforms.Resize((300,300)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ])\n",
        "\n",
        "\n",
        "    home_path = \"/content/inaturalist_12K\"\n",
        "\n",
        "    train_path = os.path.join(home_path,'train')\n",
        "    test_path = os.path.join(home_path,'val')\n",
        "    # define the dataset and apply the transforms\n",
        "    train_dataset = ImageFolder(train_path, transform=train_transforms)\n",
        "    test_dataset = ImageFolder(test_path, transform=test_transforms)\n",
        "\n",
        "    # split training dataset into train and validation sets\n",
        "    train_size = int(0.8 * len(train_dataset))\n",
        "    print(train_size)\n",
        "    val_size = len(train_dataset) - train_size\n",
        "    print(val_size)\n",
        "\n",
        "    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "    # create a data loader for the training data\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,bs, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, bs, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, bs, shuffle=False)\n",
        "\n",
        "    #categories\n",
        "    root=pathlib.Path(train_path)\n",
        "\n",
        "    classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
        "\n",
        "    return train_loader,val_loader,test_loader,classes\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5f5f5b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:19:48.157834Z",
          "iopub.status.busy": "2023-04-07T11:19:48.157506Z",
          "iopub.status.idle": "2023-04-07T11:19:48.181396Z",
          "shell.execute_reply": "2023-04-07T11:19:48.180429Z"
        },
        "papermill": {
          "duration": 0.0347,
          "end_time": "2023-04-07T11:19:48.183608",
          "exception": false,
          "start_time": "2023-04-07T11:19:48.148908",
          "status": "completed"
        },
        "tags": [],
        "id": "7c5f5f5b"
      },
      "outputs": [],
      "source": [
        "#Simple CNN\n",
        "def flatten(k=[11,9,7,5,3],w=300, s=1, p=1):\n",
        "    r=w\n",
        "    for i in  range(len(k)):\n",
        "        print(\"r\",r)\n",
        "        r= (r+2*p-k[i])+1\n",
        "        r= int(r/2)+1\n",
        "    return r\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_class=10,num_filters=4,kernel_sizes=[11,9,7,5,3],fc_neurons=64,batch_norm=True,dropout=0.3,filter_multiplier=2,activation='LeakyRelu'):\n",
        "\n",
        "        super(CNN, self).__init__()\n",
        "        self.in_channels=in_channels\n",
        "        self.num_class=num_class\n",
        "        self.num_filters=num_filters\n",
        "        self.kernel_sizes=kernel_sizes\n",
        "        self.fc_neurons=fc_neurons\n",
        "        self.activation=activation\n",
        "        self.batch_norm=batch_norm\n",
        "        self.dropout=dropout\n",
        "        self.filter_multiplier=filter_multiplier\n",
        "\n",
        "        #print(\"in1\")\n",
        "        self.conv1 = nn.Conv2d(3, num_filters, kernel_size=kernel_sizes[0],stride=1, padding=1).to(device)\n",
        "        #print(\"in2\")\n",
        "        self.bn1=nn.BatchNorm2d(num_features=num_filters)\n",
        "        self.relu1 = nn.LeakyReLU()\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(num_filters,filter_multiplier*num_filters,  kernel_size=kernel_sizes[1],stride=1, padding=1).to(device)\n",
        "        self.bn2=nn.BatchNorm2d(num_features=filter_multiplier*num_filters)\n",
        "        self.relu2 = nn.LeakyReLU()\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(filter_multiplier*num_filters, int(math.pow(filter_multiplier, 2))*num_filters, kernel_size=kernel_sizes[2],stride=1, padding=1).to(device)\n",
        "        self.bn3=nn.BatchNorm2d(num_features=int(math.pow(filter_multiplier, 2))*num_filters)\n",
        "        self.relu3 = nn.LeakyReLU()\n",
        "\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(int(math.pow(filter_multiplier, 2))*num_filters, int(math.pow(filter_multiplier, 3))*num_filters, kernel_size=kernel_sizes[3],stride=1, padding=1).to(device)\n",
        "        self.bn4=nn.BatchNorm2d(num_features=int(math.pow(filter_multiplier, 3))*num_filters)\n",
        "        self.relu4 = nn.LeakyReLU()\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(int(math.pow(filter_multiplier, 3))*num_filters, int(math.pow(filter_multiplier, 4))*num_filters, kernel_size=kernel_sizes[4],stride=1, padding=1).to(device)\n",
        "        self.bn5=nn.BatchNorm2d(num_features=int(math.pow(filter_multiplier, 4))*num_filters)\n",
        "        self.relu5 = nn.LeakyReLU()\n",
        "\n",
        "\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
        "        print(\"ok pool5\")\n",
        "        self.r=flatten(kernel_sizes)\n",
        "        print(\"ok flatten\")\n",
        "        print(self.r)\n",
        "        self.fc1 = nn.Linear(in_features=int(math.pow(filter_multiplier, 4))*num_filters*self.r*self.r, out_features=fc_neurons)\n",
        "        self.relu6 = nn.LeakyReLU()\n",
        "\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc2 = nn.Linear(in_features=fc_neurons, out_features=num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        if self.batch_norm:\n",
        "            x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        if self.batch_norm:\n",
        "            x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        if self.batch_norm:\n",
        "            x = self.bn3(x)\n",
        "        x = self.relu3(x)\n",
        "\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        if self.batch_norm:\n",
        "            x = self.bn4(x)\n",
        "        x = self.relu4(x)\n",
        "\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        if self.batch_norm:\n",
        "            x = self.bn5(x)\n",
        "        x = self.relu5(x)\n",
        "\n",
        "        x = self.pool5(x)\n",
        "        x = x.view(x.size(0),-1)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu6(x)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "173205f4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:19:48.200190Z",
          "iopub.status.busy": "2023-04-07T11:19:48.199914Z",
          "iopub.status.idle": "2023-04-07T11:50:32.615877Z",
          "shell.execute_reply": "2023-04-07T11:50:32.614873Z"
        },
        "papermill": {
          "duration": 1844.430252,
          "end_time": "2023-04-07T11:50:32.621794",
          "exception": false,
          "start_time": "2023-04-07T11:19:48.191542",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "173205f4",
        "outputId": "d0d7458c-6f52-4922-95b0-d667b11de6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7999\n",
            "2000\n",
            "['.DS_Store', 'Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n",
            "Feature Batch Shape: torch.Size([64, 3, 300, 300])\n",
            "Label Batch Shape: torch.Size([64])\n",
            "ok pool5\n",
            "r 300\n",
            "r 151\n",
            "r 76\n",
            "r 39\n",
            "r 20\n",
            "ok flatten\n",
            "11\n",
            "Epoch [1/15], Train Loss: 2.1334, Test Loss: 2.1505, Test Acc: 21.35%\n",
            "Epoch [2/15], Train Loss: 1.9480, Test Loss: 1.9612, Test Acc: 31.30%\n",
            "Epoch [3/15], Train Loss: 2.0581, Test Loss: 1.9284, Test Acc: 32.50%\n",
            "Epoch [4/15], Train Loss: 1.8285, Test Loss: 1.8730, Test Acc: 34.55%\n",
            "Epoch [5/15], Train Loss: 1.8385, Test Loss: 1.8725, Test Acc: 34.05%\n",
            "Epoch [6/15], Train Loss: 1.6753, Test Loss: 1.8064, Test Acc: 37.25%\n",
            "Epoch [7/15], Train Loss: 1.7379, Test Loss: 1.7734, Test Acc: 36.40%\n",
            "Epoch [8/15], Train Loss: 1.7229, Test Loss: 1.7558, Test Acc: 38.50%\n",
            "Epoch [9/15], Train Loss: 1.5215, Test Loss: 1.7803, Test Acc: 37.50%\n",
            "Epoch [10/15], Train Loss: 1.4917, Test Loss: 1.7439, Test Acc: 39.40%\n",
            "Epoch [11/15], Train Loss: 1.5123, Test Loss: 1.7671, Test Acc: 38.90%\n",
            "Epoch [12/15], Train Loss: 1.3941, Test Loss: 1.7890, Test Acc: 38.10%\n",
            "Epoch [13/15], Train Loss: 1.4545, Test Loss: 1.8436, Test Acc: 38.80%\n",
            "Epoch [14/15], Train Loss: 1.2845, Test Loss: 1.8135, Test Acc: 39.70%\n",
            "Epoch [15/15], Train Loss: 1.3934, Test Loss: 1.8889, Test Acc: 38.80%\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "in_channels = 3\n",
        "num_class = 10\n",
        "learning_rate = 0.0005\n",
        "batch_size = 64\n",
        "epochs = 15\n",
        "data_aug=True\n",
        "\n",
        "# Load data\n",
        "train_loader,val_loader,test_loader,classes=load_data(batch_size,data_aug)\n",
        "print(classes)\n",
        "trainfeature, trainlabel = next(iter(train_loader))\n",
        "print(f\"Feature Batch Shape: {trainfeature.size()}\")\n",
        "print(f\"Label Batch Shape: {trainlabel.size()}\")\n",
        "\n",
        "\n",
        "\n",
        "# Initialize network\n",
        "model = CNN(3,10,16,[3,3,3,3,3],128,False,0,2,'LeakyRelu').to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer=optim.NAdam(model.parameters(),lr=learning_rate,weight_decay=0.0001)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # backward\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Track the total loss and number of correct predictions\n",
        "    test_loss = 0\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "\n",
        "            scores = model(data)\n",
        "            test_loss += criterion(scores, targets).item()\n",
        "\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == targets).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    # Calculate the average validation loss and accuracy\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = float(num_correct) / num_samples\n",
        "\n",
        "    # Print the epoch number, loss, and accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, epochs, loss.item(), test_loss, test_acc*100))\n",
        "\n",
        "# Check accuracy on training & test to see how good our model\n",
        "# Save best model\n",
        "best_model_path = 'best_model.pth'\n",
        "torch.save(model.state_dict(), best_model_path)\n",
        "print(f\"Best model saved to {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "in_channels = 3\n",
        "num_class = 10\n",
        "learning_rate = 0.0001\n",
        "batch_size = 32\n",
        "epochs = 15\n",
        "data_aug=False\n",
        "\n",
        "# Load data\n",
        "train_loader,val_loader,test_loader,classes=load_data(batch_size,data_aug)\n",
        "print(classes)\n",
        "trainfeature, trainlabel = next(iter(train_loader))\n",
        "print(f\"Feature Batch Shape: {trainfeature.size()}\")\n",
        "print(f\"Label Batch Shape: {trainlabel.size()}\")\n",
        "\n",
        "\n",
        "\n",
        "# Initialize network\n",
        "model = CNN(3,10,16,[7,5,5,3,3],64,True,0.2,2,'Mish').to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer=optim.NAdam(model.parameters(),lr=learning_rate,weight_decay=0.0001)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # forward\n",
        "        scores = model(data)\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # backward\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Track the total loss and number of correct predictions\n",
        "    test_loss = 0\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    with torch.no_grad():\n",
        "        for data, targets in test_loader:\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "\n",
        "            scores = model(data)\n",
        "            test_loss += criterion(scores, targets).item()\n",
        "\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == targets).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    # Calculate the average validation loss and accuracy\n",
        "    test_loss /= len(test_loader)\n",
        "    test_acc = float(num_correct) / num_samples\n",
        "\n",
        "    # Print the epoch number, loss, and accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, epochs, loss.item(), test_loss, test_acc*100))\n",
        "\n",
        "# Check accuracy on training & test to see how good our model\n",
        "# Save best model\n",
        "best_model_path = 'best_model.pth'\n",
        "torch.save(model.state_dict(), best_model_path)\n",
        "print(f\"Best model saved to {best_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu9Xrbjq55Io",
        "outputId": "f474cc00-6cbe-4073-994c-c8dfee32e86b"
      },
      "id": "Uu9Xrbjq55Io",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7999\n",
            "2000\n",
            "['.DS_Store', 'Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n",
            "Feature Batch Shape: torch.Size([32, 3, 300, 300])\n",
            "Label Batch Shape: torch.Size([32])\n",
            "ok pool5\n",
            "r 300\n",
            "r 149\n",
            "r 74\n",
            "r 37\n",
            "r 19\n",
            "ok flatten\n",
            "10\n",
            "Epoch [1/15], Train Loss: 2.0479, Test Loss: 2.0485, Test Acc: 28.80%\n",
            "Epoch [2/15], Train Loss: 1.9781, Test Loss: 1.9840, Test Acc: 30.55%\n",
            "Epoch [3/15], Train Loss: 1.8830, Test Loss: 1.8953, Test Acc: 33.50%\n",
            "Epoch [4/15], Train Loss: 1.5873, Test Loss: 1.8801, Test Acc: 34.70%\n",
            "Epoch [5/15], Train Loss: 1.6135, Test Loss: 1.8147, Test Acc: 37.05%\n",
            "Epoch [6/15], Train Loss: 1.8353, Test Loss: 1.8832, Test Acc: 34.05%\n",
            "Epoch [7/15], Train Loss: 1.6605, Test Loss: 1.8454, Test Acc: 36.95%\n",
            "Epoch [8/15], Train Loss: 1.2188, Test Loss: 1.8422, Test Acc: 37.30%\n",
            "Epoch [9/15], Train Loss: 1.6026, Test Loss: 1.8820, Test Acc: 35.75%\n",
            "Epoch [10/15], Train Loss: 1.8103, Test Loss: 1.8908, Test Acc: 37.45%\n",
            "Epoch [11/15], Train Loss: 1.2272, Test Loss: 1.7735, Test Acc: 40.15%\n",
            "Epoch [12/15], Train Loss: 1.2077, Test Loss: 1.8376, Test Acc: 38.05%\n",
            "Epoch [13/15], Train Loss: 1.4188, Test Loss: 1.8078, Test Acc: 38.30%\n",
            "Epoch [14/15], Train Loss: 1.1272, Test Loss: 1.7984, Test Acc: 38.95%\n",
            "Epoch [15/15], Train Loss: 1.1612, Test Loss: 1.9012, Test Acc: 39.10%\n",
            "Best model saved to best_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the best model and testing it on Test Data\n",
        "best_model_path = 'best_model.pth'\n",
        "\n",
        "loaded_model = CNN(3,10,16,[7,5,5,3,3],64,True,0.2,2,'Mish').to(device)\n",
        "loaded_model.load_state_dict(torch.load(best_model_path)) # it takes the loaded dictionary, not the path file itself\n",
        "\n",
        "def calculate_accuracy(model, test_loader,criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    cost=0\n",
        "    acc=0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            cost +=criterion(outputs,labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images\n",
        "            del labels\n",
        "    acc=100 * correct / total\n",
        "    cost/=len(test_loader)\n",
        "\n",
        "    return cost,acc\n",
        "loss,acc=calculate_accuracy(loaded_model,test_loader,nn.CrossEntropyLoss())\n",
        "print(loss,acc)\n",
        "\n",
        "print(loaded_model.state_dict())\n"
      ],
      "metadata": {
        "id": "DlB9WFER0we3"
      },
      "id": "DlB9WFER0we3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path = 'best_model.pth'\n",
        "\n",
        "loaded_model = CNN(3,10,16,[7,5,5,3,3],64,True,0.2,2,'Mish')\n",
        "loaded_model.load_state_dict(torch.load(best_model_path)) # it takes the loaded dictionary, not the path file itself\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"CS6910_Assignment_2_Q2\")\n",
        "\n",
        "\n",
        "# Define a function to generate predictions and sample images from the test data\n",
        "def generate_predictions(model, data_loader):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a list to store the predictions and sample images\n",
        "    predictions = []\n",
        "    sample_images = []\n",
        "\n",
        "    # Generate predictions and sample images\n",
        "    with torch.no_grad():\n",
        "        for batch, _ in data_loader:\n",
        "            # Forward pass through the model\n",
        "            output = model(batch)\n",
        "\n",
        "            # Get the predicted class labels\n",
        "            _, predicted = torch.max(output, 1)\n",
        "\n",
        "            # Convert the predicted labels to image tensors\n",
        "            predicted_images = torchvision.utils.make_grid(batch[predicted])\n",
        "\n",
        "            # Append the predictions and sample images to the lists\n",
        "            predictions.append(predicted_images)\n",
        "            sample_images.append(torchvision.utils.make_grid(batch))\n",
        "\n",
        "    # Concatenate the predictions and sample images into grids\n",
        "    prediction_grid = torchvision.utils.make_grid(predictions, nrow=3)\n",
        "    sample_grid = torchvision.utils.make_grid(sample_images, nrow=3)\n",
        "\n",
        "    # Return the grids\n",
        "    return prediction_grid, sample_grid\n",
        "\n",
        "# Generate the prediction and sample image grids\n",
        "prediction_grid, sample_grid = generate_predictions(loaded_model, test_loader)\n",
        "\n",
        "# Log the grids to wandb\n",
        "wandb.log({\n",
        "    'Predictions': wandb.Image(prediction_grid),\n",
        "    'Sample Images': wandb.Image(sample_grid)\n",
        "})\n",
        "\n",
        "# Finish the run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "yoVD_qthFg6D"
      },
      "id": "yoVD_qthFg6D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703c4d76",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:50:32.649484Z",
          "iopub.status.busy": "2023-04-07T11:50:32.649023Z",
          "iopub.status.idle": "2023-04-07T11:50:52.669256Z",
          "shell.execute_reply": "2023-04-07T11:50:52.667908Z"
        },
        "papermill": {
          "duration": 20.037041,
          "end_time": "2023-04-07T11:50:52.672195",
          "exception": false,
          "start_time": "2023-04-07T11:50:32.635154",
          "status": "completed"
        },
        "tags": [],
        "id": "703c4d76",
        "outputId": "591c13b8-4a6d-4ede-fb72-f2f5352edfd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/vplab/.netrc\r\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from signal import signal,SIGPIPE, SIG_DFL\n",
        "signal(SIGPIPE,SIG_DFL)\n",
        "!pip install wandb -qU\n",
        "import wandb\n",
        "!wandb login --relogin 3d199b9bde866b3494cda2f8bb7c7a633c9fdade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be445fb4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:50:52.695381Z",
          "iopub.status.busy": "2023-04-07T11:50:52.695007Z",
          "iopub.status.idle": "2023-04-07T11:50:52.715754Z",
          "shell.execute_reply": "2023-04-07T11:50:52.714604Z"
        },
        "papermill": {
          "duration": 0.033186,
          "end_time": "2023-04-07T11:50:52.717717",
          "exception": false,
          "start_time": "2023-04-07T11:50:52.684531",
          "status": "completed"
        },
        "tags": [],
        "id": "be445fb4"
      },
      "outputs": [],
      "source": [
        "\n",
        "sweep_config = {\n",
        "    \"name\" : \"DA6401_Assignment_2\",\n",
        "    \"method\" : \"bayes\",\n",
        "    'metric': {\n",
        "        'name': 'val_acc',\n",
        "        'goal': 'maximize'\n",
        "    },\n",
        "    \"parameters\" : {\n",
        "        \"optimizer\" : {\n",
        "            \"values\" : ['adam','nadam','sgd']\n",
        "        },\n",
        "        \"activation\" : {\n",
        "            \"values\" : ['LeakyRelu','Selu','Gelu','Mish']\n",
        "        },\n",
        "        \"batch_size\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        'learning_rate':{\n",
        "            \"values\": [0.001,0.0001,0.0003,0.0005]\n",
        "        },\n",
        "        \"dropout\": {\n",
        "            \"values\": [0,0.2,0.3]\n",
        "        },\n",
        "        \"batch_norm\": {\n",
        "              \"values\": [True,False]\n",
        "        },\n",
        "        \"data_aug\": {\n",
        "              \"values\": [True,False]\n",
        "        },\n",
        "        'kernel_sizes':{\n",
        "            'values': [[3,3,3,3,3],[5,5,5,5,5],[7,5,5,3,3], [11,9,7,5,3]]\n",
        "        },\n",
        "        'filter_multiplier': {\n",
        "            'values': [1, 2, 0.5]\n",
        "        },\n",
        "        'num_filters': {\n",
        "            'values': [4,8,16]\n",
        "        },\n",
        "        \"fc_neurons\": {\n",
        "              \"values\": [32, 64, 128]\n",
        "          }\n",
        "    }\n",
        "}\n",
        "def opti(model,opt='adam',lr=0.0005):\n",
        "    print(\"in opti\")\n",
        "    if opt == \"sgd\":\n",
        "        opt= optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    elif opt == \"adam\":\n",
        "        opt = optim.Adam(model.parameters(),lr=lr,weight_decay=0.0001)\n",
        "    elif opt == \"nadam\":\n",
        "        opt = optim.NAdam(model.parameters(),lr=lr,weight_decay=0.0001)\n",
        "    print('exit opti')\n",
        "    return opt\n",
        "\n",
        "def calculate_accuracy(model, test_loader,criterion):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    cost=0\n",
        "    acc=0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            cost +=criterion(outputs,labels).item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images\n",
        "            del labels\n",
        "    acc=100 * correct / total\n",
        "    cost/=len(test_loader)\n",
        "\n",
        "    return cost,acc\n",
        "def train():\n",
        "    config_default={\n",
        "      'epochs':15,\n",
        "      'batch_size':32,\n",
        "      'learning_rate':0.001,\n",
        "      'dropout':0.3,\n",
        "      'batch_norm':True,\n",
        "      'data_aug':True,\n",
        "      'kernel_sizes':[5,5,5,5,5],\n",
        "      'filter_multiplier': 2,\n",
        "      'num_filters': 16,\n",
        "      \"fc_neurons\": 64\n",
        "  }\n",
        "    wandb.init(config=config_default)\n",
        "    c= wandb.config\n",
        "    name = \"nfliter_\"+str(c.num_filters)+\"op_\"+str(c.optimizer)+\"_ac_\"+str(c.activation)+\"_n_\"+str(c.learning_rate)+\"_bs_\"+str(c.batch_size)+\"_dp_\"+str(c.dropout)+\"_bn_\"+str(c.batch_norm)\n",
        "\n",
        "    wandb.init(name=name)\n",
        "\n",
        "    # Retrieve the hyperparameters from the config\n",
        "    lr = c.learning_rate\n",
        "    bs = c.batch_size\n",
        "    epochs = 15\n",
        "    act= c.activation\n",
        "    opt= c.optimizer\n",
        "\n",
        "    dp = c.dropout\n",
        "    bn = c.batch_norm\n",
        "    da=c.data_aug\n",
        "    ks=c.kernel_sizes\n",
        "    fm=c.filter_multiplier\n",
        "    nf=c.num_filters\n",
        "    fc=c.fc_neurons\n",
        "\n",
        "\n",
        "    # Load the dataset\n",
        "    train_loader,val_loader,test_loader,classes=load_data(bs,da)\n",
        "\n",
        "    print(\"data loaded ====================================================\")\n",
        "\n",
        "    # Initialize network\n",
        "    model= CNN(in_channels=3, num_class=10,num_filters=nf,kernel_sizes=ks,fc_neurons=fc,batch_norm=bn,dropout=dp,filter_multiplier=fm,activation=act).to(device)\n",
        "    print(\"model ini==============================================================\")\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer=opti(model,opt,lr)\n",
        "    print(\"done\")\n",
        "    # Train Network\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch enter')\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "            # Get data to cuda if possible\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # forward\n",
        "            scores = model(data)\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # backward\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # gradient descent or adam step\n",
        "            optimizer.step()\n",
        "            del data\n",
        "            del targets\n",
        "\n",
        "        # Calculate the test accuracy\n",
        "        train_loss,train_acc = calculate_accuracy(model, train_loader,criterion)\n",
        "        val_loss,val_acc = calculate_accuracy(model, val_loader,criterion)\n",
        "        test_loss,test_acc = calculate_accuracy(model, test_loader,criterion)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        # Log the metrics to WandB\n",
        "        wandb.log({'epoch': epoch+1,'loss':loss.item(), 'train_loss': loss.item(),'test_loss':test_loss,'val_loss':val_loss,'test_acc': test_acc,'train_acc': train_acc,'val_acc': val_acc})\n",
        "\n",
        "\n",
        "    # Save the best model\n",
        "    wandb.save('model.h5')\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47928aa2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-04-07T11:50:52.735356Z",
          "iopub.status.busy": "2023-04-07T11:50:52.735029Z",
          "iopub.status.idle": "2023-04-07T17:11:27.821419Z",
          "shell.execute_reply": "2023-04-07T17:11:27.820420Z"
        },
        "papermill": {
          "duration": 19235.097974,
          "end_time": "2023-04-07T17:11:27.823909",
          "exception": false,
          "start_time": "2023-04-07T11:50:52.725935",
          "status": "completed"
        },
        "tags": [],
        "id": "47928aa2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_2')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25641c79",
      "metadata": {
        "papermill": {
          "duration": 0.084995,
          "end_time": "2023-04-07T17:11:27.931145",
          "exception": false,
          "start_time": "2023-04-07T17:11:27.846150",
          "status": "completed"
        },
        "tags": [],
        "id": "25641c79"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_2')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77dac26f",
      "metadata": {
        "id": "77dac26f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_2')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7393ba6e",
      "metadata": {
        "id": "7393ba6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the WandB sweep\n",
        "sweep_id = wandb.sweep(sweep_config, project='DA6401_Assignment_2')\n",
        "wandb.agent(sweep_id, function=train,count=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a699f0",
      "metadata": {
        "id": "d2a699f0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 21165.436974,
      "end_time": "2023-04-07T17:11:30.816189",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-04-07T11:18:45.379215",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}